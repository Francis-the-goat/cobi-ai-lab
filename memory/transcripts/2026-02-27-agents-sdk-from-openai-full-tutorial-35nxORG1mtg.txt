today we're going to be taking a look at open ai's brand new agents SDK now agents SDK is open ai's version of a gen agents framework similar to line chain pantic Ai and so on now let's start by jumping straight into their do study just outline a few things here which we'll be covering in a actual code example I'm going to be taking through everything so the mention here that is a production ready upgrade of the previous experiment for agents which is their swarm library and they've added a few things that I think are quite useful so of course agents there is tool use you're also able to pass off from One agent to another you have input and output guard rails and it's generally generally a well-built library it still has some limitations that I think most agent Frameworks are falling into at the moment which is very strict definition of what an agent actually is but for the most part I actually do think this is a good framework now let's take a look at the code we're working through this example there is a link to this in the video description and also the comments below the video so you can go ahead and open that and and follow along with me so here I just outline this is actually coming from their docks which is over here these are the main features of the SDK okay agent Loop python first handoffs guard rails function tools and tracing we're going to covering all of these except from handoffs and tracing I'll leave those for later but yeah let's jump into those so first we are just going to install the library and we'll also of course need an open AI API key although note that this is open source so technically I think there shouldn't be any reason why we can't use this framework with other llms although I'm sure I have made that more difficult than it needs to be so we need our API key we go to platform. open.com you of course will need an account if you don't already although I'm sure most of you do and we'll need to go to API keys and just create a new secret key I'm going to call it agents SDK you call it whatever you want of course okay I'm going to copy that and come over to here and just paste it in here so now we're all sorted know what API key is great now let's just take a look at the essentials okay so there is the agent and the runner so we initialize a very simple agent here we give it a name I'm going to call it a system a very very simple system prompt here you are helpful the assistant and I'm using GPT 40 mini now running our agent there are a few methods for doing this all of these are through this Runner class so we have runner. run which we'll be using a fair bit which is just running our agent in async but without streaming then there is a runner run sync if you need to run your agent synchronously rather than asynchronously and then there is Runner run streamed which is going to run in async and also stream the response back to us we will be not using run sync but we will be using run stream and run generally speaking I would there are not many scenarios where I would ever recommend anyone to run AI applications synchronously and I've spoken about that a lot before I won't I won't talk about it again here but anyway let's try out async run method and say tell me a short story that will take a moment we're not streaming so we don't actually see anything until the whole response has been generated okay and we get our response pretty standard I think there's a ton to say about that now in most production scenarios I think you're going to be using method 3 which is the run synchronously with streaming and the reason I say that is because in the outward facing user application of where is your building you are probably going to want to one use async because async is essentially just not blocking your API if you're using if you're implementing this behind API it makes your code more scalable efficient so on and so on and two you are probably going to use streaming at least if this LM call is user facing in any way and the reason I say that is well we just ran this and we have to wait quite a while for this to actually show us anything which is going to be a bad use experience in most cases so what we want to do is just start streaming tokens as soon as we get them so the user sees that something is happening that also allows us as we I will show you in a moment to stream tool use updates which I think are incredibly useful so we're going to run streamed input here is hello there and we're just going to print every single event that is returned to us now this is going be a lot of information okay so we can see there's a lot of stuff there basically for every type of event so there is an event for we have we are using this new agent this current agent that has its own event then there's the stream events so these are the tokens that are being generated by your llm or updates of okay I'm going to use this tool or this tool so on and then we also have this final one here this run item stream event which is telling as okay the llm is finished or the agent llm whatever has finished generating its message output okay and if we look at these objects there is quite a lot of information in there so we need to we need to pass that out and make it a little bit easier to understand which we can do quite easily fortunately so first I'm just going to show you how we can get the raw tokens which is we look for the event type and we say if it's a raw response event that is the LM generated tokens streamed back to us okay and you can see we get straight away it's streaming that's pretty that's pretty nice okay but this is only going to work for a direct llm agent output as soon as we start introducing tools things get a little more complicated so how do we well let's see I'll show you how to how we do that now you can see that open AI have called their tool calling a function tool so and no open a I started with function calling when they first introduced the concept into their apis it was called function calling then they decided it's not called function calling it's instead called tool calling and now it seems they have decided they don't know which one they like the most so it's now the function tool so thank you open the eye for the conciseness and Clarity there the way that we use or Define tools I'm just going to call them tools is how we would in most other AI Frameworks to be honest it's not complicated so I'm defining a simple tool here it's a multiply tool which going to take a value of float X float Y and multiply them together super simple I have a do string here this is natural language describing to the llm or agent what this tool does and you can also put instructions on how to use tools in these doct strings as well if needed and you can see that we're being very precise in our type annotations here describing what everything is essentially providing as much information to our agent as possible then we decorate that function with the function tool decorator from the agent SDK and that is how we Define a tool it's very simple so we have our tool now how do we we run our agent with that tool again not difficult we simply pass the tool within list to the tools parameter during our agent definition I also added a little more to the system prompt SL instructions here I just added do not rely on your own knowledge too much and instead use your tools to help you answer queries so I basically don't want the lm/ agent trying to do math by itself I want to use my multiply tool so we have that and now we can run it so we execute this in the exact same way as before bage going to ask it to multiply these two numbers together and of course that we would expect our agent to use the multiply tool in this scenario so we do that and I'm going to print out all events because now we have a ton of different events coming through so you can see that we have the raw responses streaming event that is covering the llm generating tokens for our tool calls and also our final response which is what we see down here we also have these run item stream events which is okay the tool is called and then also here the tool has been executed and we have the output from the actual function itself and then down here we have that okay I'm done event so we need to pass all of this in a way that makes it a lot easier to understand what is happening now that doesn't need to be that doesn't need to be super complicated but I've added a lot of stuff in here just so just so you can see how we can extract different parts of these events so this segment here this is all raw response event so these are the tokens as they are being streamed by our llm okay now this will output everything in some format for us however what the agents SDK also does for us is it provides these other events so this event here tells us which agent we are currently using because you can you can use multiple agents in a sequence so you might see this event pop up if you have one of those multi-agent workflows or if you're just running your first agent which of course we are doing here we also have this run item stream event so this includes the tool calling so where we are outputting the tokens from our llm but it waits for our entire tool call to be complete before outputting this event and in this event it just includes all of that information in one single place which is easier for us to uh pass and also within this segment we will get our tool output so this is we execute our tool function function tool however they've called it and we have that answer so the the X multiplied by y we get the answer from that that so if we run this we're going to see a much cleaner output now we're using GPT 40 mini so that is actually super fast I will slow it down for you now but you can see here that we have first the current agent we can see which agent's been used then it streams all of our tool parameter tokens then after that has completed our tool call is complete so the agent SDK outputs what tool is been called the name and the ODS and then it executes the tool and provides us the output from there then finally we stream token by token the final output great so we have that that is our streaming now we also have guard rails God rails are God rails are interesting and they're relatively open which I like because I I would Implement God rails in a slightly different way or I would like multiple ways to implement guard rils they are super important though so if you're not already using guard rails I would recommend using them more so we are first in this example just going to implement a guard rail Power by the LM now it's also worth noting that the guard rails here there are two types there is a input guard rail which I'll show you how to implement here and there is also an output guard rail which is essentially exactly the same just on the other side so the input guard rail is checking the input going into your llm and the output guard rail is checking the output from your llm that is going to your user so you can you can guard rail both sides of the conversation which again is is pretty standard practice and I I think it's important so we are going to implement a guard rail powered by another LM okay so that means we'll just be giving open AI all of our money and to do that we Implement another agent right and this this this agent's one and only job is to check if we are hitting a guard rail okay and specifically this agent is checking if the user is asking the agent about its political opinions which we don't want it to do so we Define this God output item and and discard output is being passed to the output type of our agent and what this is going to do is it's going to force the agent to provide us with a structured output so the well the agent is going to Output it's going to generate the answer within this format that we've defined here so it's going to provide us with a is triggered method which is a Boolean value so it's going to be true if the guard rail has been triggered or false if it has not been triggered then we're also going to allow it to explain to us why it thinks the guard rail has been triggered or not which you probably I think this can be useful during development you would probably want to turn it off in any production setting because you're just spending more and more tokens so yeah I it it's useful but it's useful for just understanding the reasoning of course great so we initialize that I don't think there's anything else to say there we move on on and what we can do first is just see okay does this agent work does it let's see what it outputs so I'm going to ask it what it thinks about the labor party in UK and what does it think oh we don't know because opening eye returns this mess back to us so we the the answer is in here it's just hidden and you can you can find it I think it's even in multiple places look we have is triggered true is triggered true again and I think there's another one so we have the answer multiple times in there but we need to extract it out because it is hard to read so we just feel a result final output and we get this nice pantic class guard output we have is triggered is true and we say the reasoning so the user is asking for an opinion on a political party which falls under the category of political opinions thank you so much so we have our logic now have do we Implement that in an another agent so our let's say our original agent which had the multiply tool let's go and see how we do that well we are going to need to use this input guardrail decorator on a function which basically is going to run our politics agent that we just defined get the response and then return that response via this guardrail function object okay so there's a there's a St RCT format that we need to follow here in order to implement this input guard rail with any other agents so we need our input parameters to follow this pattern right we don't even use these two in this example we're not using these we're just using this input but we have to have these two parameters otherwise this will not work it won't be inv valid guard rail and we have to Output this format so that the agents SDK knows what to do with what we're outputting okay and once we Define that we can then plug it in to another agent this other agent is exactly the same as the agent we had before which looks exactly like this but now we have just added that politics guardrail and know that input guard rails here is going to be a list of those input guard rail objects also worth noting is if you have a output guard rail it would just be like this so you'd have output guard rails and then you'd put politics guard rail or whatever else the only other difference is that up here this would be a output guard rail worth meting so let's define our new safe agent and we are going to ask it again what these two numbers multipli together are and we should see that it will answer us using the tool okay that's great so we're not blocking everything but what if we ask it about the labor partying UK again we will see an error and we would of course in our applications need to handle this error we can see that the error being raised here is expected is our input guard trip wire triggered error so that is pretty useful and that is how we use guard rails now the final thing that I do want to cover because this is obviously very important is so far we've just been feeding in a single input query a single string into our agents and there are many probably the vast majority of use cases are not going to be doing that instead they're going to be feeding in a list of interactions between user and assistant over time so how do we take what we've done so far and make our agent conversational it is fairly straightforward it's not complicated so first let's just ask our agent to remember the number 7814 for us and we remember to use our manage there and we get I cannot store or remember information for future use however you can save in note or use a reminder app thank you very much so the agent is telling us oh that we can't do that but actually we can do that the agent just doesn't know it so we come down to here agents SDK has this nice method actually which is two input list so we're taking our result here and we are converting it into an input list for our next query or our next message and we get this list of messages the first one here is the the message from us the user message where we ask it to remember that number then the next one has a lot more information but it's coming from the agent we see that the role here is and that is not the name of our agent that is just the AI message and we also have the content which includes these annotations I assume that will be for citations or something else and we have the text content which where is telling this I can't remember anything which C is okay so we actually merge that two input list here with our next message okay so our next message we are going to use a dictionary here where we specify the we are the user user this is the user message and I'm going to say multiply the last number so I'm not specifying what number it should remember now by this 103.8 N2 let's run that and we will see our final output okay so the final output is the result of multiplying those two numbers is approximately 945 00:20:57,600 --> 00:21:00,870 81.81% agent can remember our previous interactions which is great so that is actually everything I wanted to cover we've I think covered the essentials of the library there there are of course a lot of other things in there there of course the handoff the tracing and even within the features that we did just cover there is there's a lot more nuance and detail to those which I will definitely almost definitely cover pretty soon but it's definitely worth looking at the S and as I mentioned at the start I think this is up there as one of my preferred Frameworks for building agents as long as those agents are not too complicated or as long as I don't need too much flexibility in what they might look like and I think also if as long as I'm using open AI which might not always be the case so interesting framework I think generally wellb built and definitely something we'll be covering more in the future for now I'll leave it there so thank you very much for watching and I will see you again in the next one bye
