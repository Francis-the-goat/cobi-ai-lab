In January, CLA reported its AI agent In January, CLA reported its AI agent In January, CLA reported its AI agent now does the work of 853 full-time now does the work of 853 full-time now does the work of 853 full-time employees and has saved the company $60 employees and has saved the company $60 employees and has saved the company $60 million. In the same earnings cycle, its million. In the same earnings cycle, its million. In the same earnings cycle, its CEO admitted publicly that the AI CEO admitted publicly that the AI CEO admitted publicly that the AI strategy had cost something far more strategy had cost something far more strategy had cost something far more valuable than $60 million, and he's valuable than $60 million, and he's valuable than $60 million, and he's still trying to buy it back. This is not still trying to buy it back. This is not still trying to buy it back. This is not another AI is overhyped story. It is another AI is overhyped story. It is another AI is overhyped story. It is actually the opposite. The AI work too actually the opposite. The AI work too actually the opposite. The AI work too well. And the distinction between AI well. And the distinction between AI well. And the distinction between AI that fails and AI that succeeds at the that fails and AI that succeeds at the that fails and AI that succeeds at the wrong thing is the most important wrong thing is the most important wrong thing is the most important unsolved problem in enterprise AI right unsolved problem in enterprise AI right unsolved problem in enterprise AI right now. This is a bigger problem than now. This is a bigger problem than now. This is a bigger problem than context engineering per se, although context engineering per se, although context engineering per se, although that's a piece of it. It's bigger than that's a piece of it. It's bigger than that's a piece of it. It's bigger than prompt engineering, which now frankly prompt engineering, which now frankly prompt engineering, which now frankly looks like a warm-up act. I'm going to looks like a warm-up act. I'm going to looks like a warm-up act. I'm going to call what we're talking about here call what we're talking about here call what we're talking about here intent engineering. It is the discipline intent engineering. It is the discipline intent engineering. It is the discipline of making organizational purpose like of making organizational purpose like of making organizational purpose like goals, values, tradeoffs, decision goals, values, tradeoffs, decision goals, values, tradeoffs, decision boundaries. These need to become machine boundaries. These need to become machine boundaries. These need to become machine readable and machine actionable so that readable and machine actionable so that readable and machine actionable so that when you deploy an autonomous system, it when you deploy an autonomous system, it when you deploy an autonomous system, it optimizes for what your company actually optimizes for what your company actually optimizes for what your company actually needs, not just what it can measure. needs, not just what it can measure. needs, not just what it can measure. Here's the CLA backstory. In early 2024, Here's the CLA backstory. In early 2024, Here's the CLA backstory. In early 2024, Ara rolled out an AI powered customer Ara rolled out an AI powered customer Ara rolled out an AI powered customer service agent. It handled 2.3 million service agent. It handled 2.3 million service agent. It handled 2.3 million conversations in the first month across conversations in the first month across conversations in the first month across 23 markets in 35 languages. Resolution 23 markets in 35 languages. Resolution 23 markets in 35 languages. Resolution times dropped from 11 minutes to two. times dropped from 11 minutes to two. times dropped from 11 minutes to two. The CEO projected $40 million in The CEO projected $40 million in The CEO projected $40 million in savings. And then customers started savings. And then customers started savings. And then customers started complaining. Generic answers, robotic complaining. Generic answers, robotic complaining. Generic answers, robotic tone, no ability to handle anything tone, no ability to handle anything tone, no ability to handle anything requiring judgment. By mid 2025, CEO requiring judgment. By mid 2025, CEO requiring judgment. By mid 2025, CEO Sebastian Seycowski told Bloomberg that Sebastian Seycowski told Bloomberg that Sebastian Seycowski told Bloomberg that while cost was a predominant evaluation while cost was a predominant evaluation while cost was a predominant evaluation factor, the result was lower quality. factor, the result was lower quality. factor, the result was lower quality. End quote. Clarin began frantically End quote. Clarin began frantically End quote. Clarin began frantically rehiring the human agents it had gutted. rehiring the human agents it had gutted. rehiring the human agents it had gutted. Most people tell this story as proof Most people tell this story as proof Most people tell this story as proof that AI can't handle nuance, and that that AI can't handle nuance, and that that AI can't handle nuance, and that was a comforting reading in early 2025. was a comforting reading in early 2025. was a comforting reading in early 2025. A more interesting reading in 2026 is A more interesting reading in 2026 is A more interesting reading in 2026 is that the AI agent was extraordinarily that the AI agent was extraordinarily that the AI agent was extraordinarily good at resolving tickets fast and that good at resolving tickets fast and that good at resolving tickets fast and that that was the wrong goal to give the that was the wrong goal to give the that was the wrong goal to give the agent. Clara's organizational intent agent. Clara's organizational intent agent. Clara's organizational intent wasn't resolve tickets fast. It was wasn't resolve tickets fast. It was wasn't resolve tickets fast. It was actually build lasting customer actually build lasting customer actually build lasting customer relationships that drive lifetime value relationships that drive lifetime value relationships that drive lifetime value in a very competitive fintech market. in a very competitive fintech market. in a very competitive fintech market. Those are profoundly different goals and Those are profoundly different goals and Those are profoundly different goals and they require profoundly different they require profoundly different they require profoundly different decision-making at the point of decision-making at the point of decision-making at the point of interaction. A human agent with five interaction. A human agent with five interaction. A human agent with five years at the company knows this years at the company knows this years at the company knows this difference intuitively. She knows when difference intuitively. She knows when difference intuitively. She knows when to bend a policy, when to spend three to bend a policy, when to spend three to bend a policy, when to spend three extra minutes because the customer's extra minutes because the customer's extra minutes because the customer's tone says they're about to churn, when tone says they're about to churn, when tone says they're about to churn, when efficiency is the right move versus when efficiency is the right move versus when efficiency is the right move versus when generosity is the right move. She knows generosity is the right move. She knows generosity is the right move. She knows this because she absorbed Clara's real this because she absorbed Clara's real this because she absorbed Clara's real values. Not the ones on the website, but values. Not the ones on the website, but values. Not the ones on the website, but the ones encoded in the decisions the ones encoded in the decisions the ones encoded in the decisions managers make every day in the stories managers make every day in the stories managers make every day in the stories veterans tell new hires. In the veterans tell new hires. In the veterans tell new hires. In the unwritten rules about which metrics unwritten rules about which metrics unwritten rules about which metrics leadership actually cares about when leadership actually cares about when leadership actually cares about when Bush comes to shove, the AI agent knew Bush comes to shove, the AI agent knew Bush comes to shove, the AI agent knew none of it. It had a prompt. It had none of it. It had a prompt. It had none of it. It had a prompt. It had context. It did not have intent. I am context. It did not have intent. I am context. It did not have intent. I am concerned that the AI agent concerned that the AI agent concerned that the AI agent inadvertently inadvertently inadvertently reflected the real values of CLA when it reflected the real values of CLA when it reflected the real values of CLA when it behaved the way it did because the real behaved the way it did because the real behaved the way it did because the real values of CLA may have been to save the values of CLA may have been to save the values of CLA may have been to save the money first. Nevertheless, customers money first. Nevertheless, customers money first. Nevertheless, customers pushed back and called CLA back toward pushed back and called CLA back toward pushed back and called CLA back toward its own stated values. And that's been a its own stated values. And that's been a its own stated values. And that's been a good thing because the $60 million in good thing because the $60 million in good thing because the $60 million in savings since the program rolled out savings since the program rolled out savings since the program rolled out have been not nearly enough to cover the have been not nearly enough to cover the have been not nearly enough to cover the reputational damage to CLA from becoming reputational damage to CLA from becoming reputational damage to CLA from becoming a public laughingtock over AI and from a public laughingtock over AI and from a public laughingtock over AI and from aggressively pushing customer service aggressively pushing customer service aggressively pushing customer service resolutions that did not meet customer resolutions that did not meet customer resolutions that did not meet customer needs. Ironically, rolling out an AI needs. Ironically, rolling out an AI needs. Ironically, rolling out an AI without much thought, but perhaps in without much thought, but perhaps in without much thought, but perhaps in line with organizational directives, did line with organizational directives, did line with organizational directives, did more to push CLA toward its own stated more to push CLA toward its own stated more to push CLA toward its own stated values than perhaps anything else would values than perhaps anything else would values than perhaps anything else would have done. I want to be precise about have done. I want to be precise about have done. I want to be precise about this story. The point here is not to this story. The point here is not to this story. The point here is not to talk about CLA per se. The point is to talk about CLA per se. The point is to talk about CLA per se. The point is to talk about what our organizational talk about what our organizational talk about what our organizational intent is with AI agents, what our goals intent is with AI agents, what our goals intent is with AI agents, what our goals are, and how agents need to reflect the are, and how agents need to reflect the are, and how agents need to reflect the larger perspective and longer term larger perspective and longer term larger perspective and longer term challenge of our organizations as they challenge of our organizations as they challenge of our organizations as they become more sophisticated and operate become more sophisticated and operate become more sophisticated and operate for longer time scales themselves. We for longer time scales themselves. We for longer time scales themselves. We have agents now that run for multiple have agents now that run for multiple have agents now that run for multiple weeks. We have agents soon that will run weeks. We have agents soon that will run weeks. We have agents soon that will run for multiple months. We are at a level for multiple months. We are at a level for multiple months. We are at a level where it is time to think about how where it is time to think about how where it is time to think about how agents interact with organizational agents interact with organizational agents interact with organizational goals very seriously and CLA is just goals very seriously and CLA is just goals very seriously and CLA is just kind of an example of why that's kind of an example of why that's kind of an example of why that's important. I want to be precise about important. I want to be precise about important. I want to be precise about how we got here because naming things how we got here because naming things how we got here because naming things matters. Naming is how we create a matters. Naming is how we create a matters. Naming is how we create a shared understanding and I think we are shared understanding and I think we are shared understanding and I think we are short on naming things correctly when it short on naming things correctly when it short on naming things correctly when it comes to intent and context. Prompt comes to intent and context. Prompt comes to intent and context. Prompt engineering was the first discipline in engineering was the first discipline in engineering was the first discipline in the age of AI. It was individual, the age of AI. It was individual, the age of AI. It was individual, synchronous, and sessionbased. You sit synchronous, and sessionbased. You sit synchronous, and sessionbased. You sit in front of the chat window, you craft in front of the chat window, you craft in front of the chat window, you craft an instruction, you iterate the output. an instruction, you iterate the output. an instruction, you iterate the output. It's a personal skill, and the value is It's a personal skill, and the value is It's a personal skill, and the value is personal. This is the era that produced personal. This is the era that produced personal. This is the era that produced a thousand how to write the perfect a thousand how to write the perfect a thousand how to write the perfect prompt blog post. Most of them are prompt blog post. Most of them are prompt blog post. Most of them are terrible. Context engineering followed terrible. Context engineering followed terrible. Context engineering followed prompt engineering. It's the one the prompt engineering. It's the one the prompt engineering. It's the one the industry is currently grappling with. industry is currently grappling with. industry is currently grappling with. Anthropic published a foundational piece Anthropic published a foundational piece Anthropic published a foundational piece in September of 2025 that defined in September of 2025 that defined in September of 2025 that defined context engineering is the shift from context engineering is the shift from context engineering is the shift from crafting isolated instructions to crafting isolated instructions to crafting isolated instructions to crafting the entire information state crafting the entire information state crafting the entire information state that an AI system operates within. that an AI system operates within. that an AI system operates within. Chains Harrison Chase put it more Chains Harrison Chase put it more Chains Harrison Chase put it more bluntly in a Sequoia Capital interview bluntly in a Sequoia Capital interview bluntly in a Sequoia Capital interview when he described it as everything's when he described it as everything's when he described it as everything's context engineering. Context engineering context engineering. Context engineering context engineering. Context engineering is such a good term. I wish I came up is such a good term. I wish I came up is such a good term. I wish I came up with that term because it describes with that term because it describes with that term because it describes everything we've done at Langchain everything we've done at Langchain everything we've done at Langchain without knowing the term existed. End without knowing the term existed. End without knowing the term existed. End quote. That's pretty good. Context quote. That's pretty good. Context quote. That's pretty good. Context engineering is where the action is right engineering is where the action is right engineering is where the action is right now. Building rag pipelines, wiring up now. Building rag pipelines, wiring up now. Building rag pipelines, wiring up MCP servers, structuring organizational MCP servers, structuring organizational MCP servers, structuring organizational knowledge so agents can access it. It's knowledge so agents can access it. It's knowledge so agents can access it. It's necessary, but it's not sufficient. And necessary, but it's not sufficient. And necessary, but it's not sufficient. And I think the industry is about to I think the industry is about to I think the industry is about to discover that in a very expensive way. discover that in a very expensive way. discover that in a very expensive way. Intent engineering. Intent engineering Intent engineering. Intent engineering Intent engineering. Intent engineering is the third discipline and it's the one is the third discipline and it's the one is the third discipline and it's the one that almost nobody's building for yet. that almost nobody's building for yet. that almost nobody's building for yet. Context engineering tells the agents Context engineering tells the agents Context engineering tells the agents what to know. Intent engineering tells what to know. Intent engineering tells what to know. Intent engineering tells agents what to want. It's the practice agents what to want. It's the practice agents what to want. It's the practice of encoding organizational purpose into of encoding organizational purpose into of encoding organizational purpose into infrastructure, not as pros in a system infrastructure, not as pros in a system infrastructure, not as pros in a system prompt, but as structured, actionable prompt, but as structured, actionable prompt, but as structured, actionable parameters that shape how agents make parameters that shape how agents make parameters that shape how agents make decisions autonomously. It's the layer decisions autonomously. It's the layer decisions autonomously. It's the layer that would have told Clara's AI agent, that would have told Clara's AI agent, that would have told Clara's AI agent, "Yes, you can resolve this ticket in 90 "Yes, you can resolve this ticket in 90 "Yes, you can resolve this ticket in 90 seconds, but the customer has been with seconds, but the customer has been with seconds, but the customer has been with us for years, and their tone indicates us for years, and their tone indicates us for years, and their tone indicates frustration. Spend the extra time. Offer frustration. Spend the extra time. Offer frustration. Spend the extra time. Offer them a specialist." The goal is them a specialist." The goal is them a specialist." The goal is retention. Without intent engineering, retention. Without intent engineering, retention. Without intent engineering, you get what Claragot, a technically you get what Claragot, a technically you get what Claragot, a technically brilliant agent optimizing for exactly brilliant agent optimizing for exactly brilliant agent optimizing for exactly the wrong objective. You get what the wrong objective. You get what the wrong objective. You get what Deoid's 2026 state of AI in the Deoid's 2026 state of AI in the Deoid's 2026 state of AI in the enterprise report found across 3,000 enterprise report found across 3,000 enterprise report found across 3,000 some leaders in 24 countries. 84% of some leaders in 24 countries. 84% of some leaders in 24 countries. 84% of companies have not redesigned jobs companies have not redesigned jobs companies have not redesigned jobs around AI capabilities and only 21% have around AI capabilities and only 21% have around AI capabilities and only 21% have a mature model for agent governance. a mature model for agent governance. a mature model for agent governance. These numbers aren't a technology story. These numbers aren't a technology story. These numbers aren't a technology story. They're an intent failure. The models They're an intent failure. The models They're an intent failure. The models work. The context pipelines are getting work. The context pipelines are getting work. The context pipelines are getting better. What's missing is the better. What's missing is the better. What's missing is the organizational infrastructure that organizational infrastructure that organizational infrastructure that connects AI capability to organizational connects AI capability to organizational connects AI capability to organizational purpose. I cited the failure stats from purpose. I cited the failure stats from purpose. I cited the failure stats from deote above. I want to show you the deote above. I want to show you the deote above. I want to show you the other side of the ledger because the other side of the ledger because the other side of the ledger because the juxtaposition makes this all very juxtaposition makes this all very juxtaposition makes this all very disorienting. Investment in AI continues disorienting. Investment in AI continues disorienting. Investment in AI continues to be massive and accelerating. Deoid's to be massive and accelerating. Deoid's to be massive and accelerating. Deoid's tech value survey found that 57% of tech value survey found that 57% of tech value survey found that 57% of respondents were putting between 21 and respondents were putting between 21 and respondents were putting between 21 and 50% of their digital transformation 50% of their digital transformation 50% of their digital transformation budgets into AI automation. and 20% of budgets into AI automation. and 20% of budgets into AI automation. and 20% of companies were investing over half on companies were investing over half on companies were investing over half on average $700 million for a company with average $700 million for a company with average $700 million for a company with 13 billion in revenue. KPMG's Q4 AI 13 billion in revenue. KPMG's Q4 AI 13 billion in revenue. KPMG's Q4 AI pulse survey showed capital flowing ROI pulse survey showed capital flowing ROI pulse survey showed capital flowing ROI confidence rising in agents moving from confidence rising in agents moving from confidence rising in agents moving from pilots to professionalized platforms. pilots to professionalized platforms. pilots to professionalized platforms. Gartner is predicting that by 2028 15% Gartner is predicting that by 2028 15% Gartner is predicting that by 2028 15% of dayto-day work decisions will be made of dayto-day work decisions will be made of dayto-day work decisions will be made autonomously by agents. I think that autonomously by agents. I think that autonomously by agents. I think that might be low. So the money is real, the might be low. So the money is real, the might be low. So the money is real, the deployments are real, and the results deployments are real, and the results deployments are real, and the results are very much in between. 74% of are very much in between. 74% of are very much in between. 74% of companies globally report they have yet companies globally report they have yet companies globally report they have yet to see tangible value from AI. McKenzie to see tangible value from AI. McKenzie to see tangible value from AI. McKenzie found 30% of AI pilots failed to achieve found 30% of AI pilots failed to achieve found 30% of AI pilots failed to achieve scaled impact. These numbers all coexist scaled impact. These numbers all coexist scaled impact. These numbers all coexist together with the investment numbers. together with the investment numbers. together with the investment numbers. There's not really a contradiction here There's not really a contradiction here There's not really a contradiction here if you start to peel the onion and if you start to peel the onion and if you start to peel the onion and understand things more carefully. What understand things more carefully. What understand things more carefully. What we're describing when we talk about a we're describing when we talk about a we're describing when we talk about a pattern of scaled investment and pattern of scaled investment and pattern of scaled investment and somewhat mixed results on deployment is somewhat mixed results on deployment is somewhat mixed results on deployment is that organizations have solved can AI do that organizations have solved can AI do that organizations have solved can AI do this task at an individual task level this task at an individual task level this task at an individual task level and they have completely failed to solve and they have completely failed to solve and they have completely failed to solve can AI do this task in a way that serves can AI do this task in a way that serves can AI do this task in a way that serves our organizational goals at scale with our organizational goals at scale with our organizational goals at scale with appropriate judgment. That second appropriate judgment. That second appropriate judgment. That second question that's an intent engineering question that's an intent engineering question that's an intent engineering question. Look at what happened with question. Look at what happened with question. Look at what happened with Microsoft Copilot. One of the most Microsoft Copilot. One of the most Microsoft Copilot. One of the most heavily invested enterprise AI products heavily invested enterprise AI products heavily invested enterprise AI products in history. Microsoft poured billions in history. Microsoft poured billions in history. Microsoft poured billions into infrastructure, embedded AI into into infrastructure, embedded AI into into infrastructure, embedded AI into every office application, and launched every office application, and launched every office application, and launched an aggressive enterprise sales campaign. an aggressive enterprise sales campaign. an aggressive enterprise sales campaign. 85% of Fortune 500 companies adopted it, 85% of Fortune 500 companies adopted it, 85% of Fortune 500 companies adopted it, and the adoption stalled hard. Gartner and the adoption stalled hard. Gartner and the adoption stalled hard. Gartner found that only 5% of organizations found that only 5% of organizations found that only 5% of organizations moved from a co-pilot pilot to a larger moved from a co-pilot pilot to a larger moved from a co-pilot pilot to a larger scale deployment. Only about 3% of the scale deployment. Only about 3% of the scale deployment. Only about 3% of the total Microsoft 365 user base actually total Microsoft 365 user base actually total Microsoft 365 user base actually adopted Copilot as paid users. Bloomberg adopted Copilot as paid users. Bloomberg adopted Copilot as paid users. Bloomberg reported Microsoft slashing internal reported Microsoft slashing internal reported Microsoft slashing internal sales targets after the majority of sales targets after the majority of sales targets after the majority of salespeople missed their goals. Even salespeople missed their goals. Even salespeople missed their goals. Even inside companies that signed six figure inside companies that signed six figure inside companies that signed six figure co-pilot deals, employees resisted. co-pilot deals, employees resisted. co-pilot deals, employees resisted. Reddit threads are full of engineers at Reddit threads are full of engineers at Reddit threads are full of engineers at multi-billion dollar companies multi-billion dollar companies multi-billion dollar companies describing their organizations describing their organizations describing their organizations downgrading licenses because employees downgrading licenses because employees downgrading licenses because employees preferred another AI. maybe chat GPT, preferred another AI. maybe chat GPT, preferred another AI. maybe chat GPT, maybe Claude. The standard explanation maybe Claude. The standard explanation maybe Claude. The standard explanation for co-pilot struggles centers on UX for co-pilot struggles centers on UX for co-pilot struggles centers on UX problems and model quality. And those problems and model quality. And those problems and model quality. And those are definitely real issues, but they're are definitely real issues, but they're are definitely real issues, but they're not the fundamental issue. The not the fundamental issue. The not the fundamental issue. The fundamental issue is that deploying an fundamental issue is that deploying an fundamental issue is that deploying an AI tool across an organization without AI tool across an organization without AI tool across an organization without organizational intent alignment is like organizational intent alignment is like organizational intent alignment is like hiring 40,000 new employees and never hiring 40,000 new employees and never hiring 40,000 new employees and never telling them what the company does, what telling them what the company does, what telling them what the company does, what it values, or how to make decisions. You it values, or how to make decisions. You it values, or how to make decisions. You get lots of activity and not much get lots of activity and not much get lots of activity and not much productivity. You get AI usage metrics productivity. You get AI usage metrics productivity. You get AI usage metrics in a dashboard and almost no measurable in a dashboard and almost no measurable in a dashboard and almost no measurable impact on what the organization is impact on what the organization is impact on what the organization is trying to accomplish. That's not a tools trying to accomplish. That's not a tools trying to accomplish. That's not a tools problem. That's an intent gap. I want to problem. That's an intent gap. I want to problem. That's an intent gap. I want to get structural because vague handwaving get structural because vague handwaving get structural because vague handwaving about AI transformation is exactly what about AI transformation is exactly what about AI transformation is exactly what we're trying to avoid here and what so we're trying to avoid here and what so we're trying to avoid here and what so many organizations get into trouble many organizations get into trouble many organizations get into trouble doing. There is an intent gap today and doing. There is an intent gap today and doing. There is an intent gap today and it operates across three distinct it operates across three distinct it operates across three distinct layers, each one at a different layers, each one at a different layers, each one at a different altitude. Getting any one of them right altitude. Getting any one of them right altitude. Getting any one of them right is helpful. Getting all three right is is helpful. Getting all three right is is helpful. Getting all three right is the difference between having AI tools the difference between having AI tools the difference between having AI tools and having an AI native organization. and having an AI native organization. and having an AI native organization. Layer one is what I'm going to call a Layer one is what I'm going to call a Layer one is what I'm going to call a unified context infrastructure. This is unified context infrastructure. This is unified context infrastructure. This is the layer the industry is most aware of the layer the industry is most aware of the layer the industry is most aware of and it's still not really built yet. and it's still not really built yet. and it's still not really built yet. Right now, every team building agents Right now, every team building agents Right now, every team building agents rolls their own context stack. One team rolls their own context stack. One team rolls their own context stack. One team pipes Slack data through a custom rag pipes Slack data through a custom rag pipes Slack data through a custom rag pipeline. Another manually exports pipeline. Another manually exports pipeline. Another manually exports Google Docs into a vector store. A third Google Docs into a vector store. A third Google Docs into a vector store. A third built an MCP server that connects to built an MCP server that connects to built an MCP server that connects to Salesforce but not to Jira. A fourth Salesforce but not to Jira. A fourth Salesforce but not to Jira. A fourth team doesn't know the other three exist team doesn't know the other three exist team doesn't know the other three exist yet. This is what one analyst called the yet. This is what one analyst called the yet. This is what one analyst called the shadow agents problem and it mirrors the shadow agents problem and it mirrors the shadow agents problem and it mirrors the shadow IT crisis of the early cloud era shadow IT crisis of the early cloud era shadow IT crisis of the early cloud era except the stakes are much higher except the stakes are much higher except the stakes are much higher because agents don't just access data, because agents don't just access data, because agents don't just access data, they act upon it. Security and they act upon it. Security and they act upon it. Security and compliance teams can't allow arbitrary compliance teams can't allow arbitrary compliance teams can't allow arbitrary unvetted agents running on developer unvetted agents running on developer unvetted agents running on developer laptops to access critical systems like laptops to access critical systems like laptops to access critical systems like customer PII, financial data or customer PII, financial data or customer PII, financial data or healthcare records. But without healthcare records. But without healthcare records. But without sanctioned infrastructure, that is sanctioned infrastructure, that is sanctioned infrastructure, that is exactly what is happening. The model exactly what is happening. The model exactly what is happening. The model context protocol which Anthropic context protocol which Anthropic context protocol which Anthropic introduced late in 2024 and donated to introduced late in 2024 and donated to introduced late in 2024 and donated to the Linux Foundation in December of 2025 the Linux Foundation in December of 2025 the Linux Foundation in December of 2025 is the most promising attempt at is the most promising attempt at is the most promising attempt at standardization at this point. UCP has standardization at this point. UCP has standardization at this point. UCP has seen a ton of adoption. OpenAI, Google, seen a ton of adoption. OpenAI, Google, seen a ton of adoption. OpenAI, Google, Microsoft, and more than 50 enterprise Microsoft, and more than 50 enterprise Microsoft, and more than 50 enterprise partners have committed to it. It's partners have committed to it. It's partners have committed to it. It's become the de facto standard. Monthly become the de facto standard. Monthly become the de facto standard. Monthly SDK downloads are close to 100 million SDK downloads are close to 100 million SDK downloads are close to 100 million now, I think. But protocol adoption and now, I think. But protocol adoption and now, I think. But protocol adoption and organizational implementation are very organizational implementation are very organizational implementation are very different things. Having a USBC standard different things. Having a USBC standard different things. Having a USBC standard does not help if your company hasn't does not help if your company hasn't does not help if your company hasn't decided which ports to install, who decided which ports to install, who decided which ports to install, who maintains them, or what gets plugged in. maintains them, or what gets plugged in. maintains them, or what gets plugged in. The context infrastructure question is The context infrastructure question is The context infrastructure question is not really a technical question. You can not really a technical question. You can not really a technical question. You can configure MCP servers. It is configure MCP servers. It is configure MCP servers. It is architectural and political. Which architectural and political. Which architectural and political. Which systems become agent accessible? Who systems become agent accessible? Who systems become agent accessible? Who decides what context an agent can see decides what context an agent can see decides what context an agent can see across departments? How do you version across departments? How do you version across departments? How do you version organizational knowledge so agents organizational knowledge so agents organizational knowledge so agents aren't operating on stale information? aren't operating on stale information? aren't operating on stale information? How do you handle the fact that the How do you handle the fact that the How do you handle the fact that the sales team Slack context and the sales team Slack context and the sales team Slack context and the engineering team Slack context encode engineering team Slack context encode engineering team Slack context encode completely different institutional completely different institutional completely different institutional assumptions? Deoid's 2025 survey found assumptions? Deoid's 2025 survey found assumptions? Deoid's 2025 survey found that nearly half of organizations cited that nearly half of organizations cited that nearly half of organizations cited data searchability and data reusability data searchability and data reusability data searchability and data reusability as top challenges blocking AI as top challenges blocking AI as top challenges blocking AI automation. I'm surprised that number automation. I'm surprised that number automation. I'm surprised that number isn't higher. As their analyst put it, isn't higher. As their analyst put it, isn't higher. As their analyst put it, the shift required is from traditional the shift required is from traditional the shift required is from traditional ETL data pipelines to enterprise search ETL data pipelines to enterprise search ETL data pipelines to enterprise search and indexing, similar to how Google made and indexing, similar to how Google made and indexing, similar to how Google made the worldwide web discover. The data the worldwide web discover. The data the worldwide web discover. The data does exist inside corporations. The does exist inside corporations. The does exist inside corporations. The agents also exist increasingly, but the agents also exist increasingly, but the agents also exist increasingly, but the connective tissue between them, the connective tissue between them, the connective tissue between them, the organizational context layer and the organizational context layer and the organizational context layer and the structures and safeguards to ensure structures and safeguards to ensure structures and safeguards to ensure that's accessed correctly, that mostly that's accessed correctly, that mostly that's accessed correctly, that mostly doesn't exist. Now, we're going to move doesn't exist. Now, we're going to move doesn't exist. Now, we're going to move on to layer two, the coherent AI worker on to layer two, the coherent AI worker on to layer two, the coherent AI worker toolkit. So, everyone's rolling out toolkit. So, everyone's rolling out toolkit. So, everyone's rolling out their own AI workflow. One person uses their own AI workflow. One person uses their own AI workflow. One person uses Claude for research and chat GPT for Claude for research and chat GPT for Claude for research and chat GPT for drafting. Another uses cursor for code drafting. Another uses cursor for code drafting. Another uses cursor for code and perplexity for factchecking. A third and perplexity for factchecking. A third and perplexity for factchecking. A third has built a custom agent chain using has built a custom agent chain using has built a custom agent chain using langraph. A fourth is copy pasting into langraph. A fourth is copy pasting into langraph. A fourth is copy pasting into a chat window. None of these employees a chat window. None of these employees a chat window. None of these employees can articulate their workflow in a way can articulate their workflow in a way can articulate their workflow in a way that's transferable, measurable, or that's transferable, measurable, or that's transferable, measurable, or improvable by anybody else. And this improvable by anybody else. And this improvable by anybody else. And this matters because the difference between matters because the difference between matters because the difference between individual AI use and organizational AI individual AI use and organizational AI individual AI use and organizational AI leverage is enormous. It's the leverage is enormous. It's the leverage is enormous. It's the difference between having one good hire difference between having one good hire difference between having one good hire and having a system that makes everybody and having a system that makes everybody and having a system that makes everybody better. It's the difference I've been better. It's the difference I've been better. It's the difference I've been writing about for a year between AI writing about for a year between AI writing about for a year between AI activity and AI fluency. The former has activity and AI fluency. The former has activity and AI fluency. The former has 30% gains that you get from bolting AI 30% gains that you get from bolting AI 30% gains that you get from bolting AI onto existing workflows. And the latter onto existing workflows. And the latter onto existing workflows. And the latter has the 300% gains you get from has the 300% gains you get from has the 300% gains you get from rethinking the workflow itself around AI rethinking the workflow itself around AI rethinking the workflow itself around AI capabilities. But here's what we need to capabilities. But here's what we need to capabilities. But here's what we need to realize. Fluency doesn't scale through realize. Fluency doesn't scale through realize. Fluency doesn't scale through training alone. It scales through shared training alone. It scales through shared training alone. It scales through shared infrastructure. Whether any individual infrastructure. Whether any individual infrastructure. Whether any individual person has Slack doesn't matter. Whether person has Slack doesn't matter. Whether person has Slack doesn't matter. Whether an agent can search 50 people's Slack an agent can search 50 people's Slack an agent can search 50 people's Slack context plus their docs plus their context plus their docs plus their context plus their docs plus their project plan plus the customer data, project plan plus the customer data, project plan plus the customer data, that's what determines whether the agent that's what determines whether the agent that's what determines whether the agent can do organizational scale work rather can do organizational scale work rather can do organizational scale work rather than individual scale tasks. Lloyd's than individual scale tasks. Lloyd's than individual scale tasks. Lloyd's 2026 report found that workforce access 2026 report found that workforce access 2026 report found that workforce access to sanctioned AI tools expanded by 50% to sanctioned AI tools expanded by 50% to sanctioned AI tools expanded by 50% in a year. But that doesn't mean that in a year. But that doesn't mean that in a year. But that doesn't mean that access is sufficient. Organizations are access is sufficient. Organizations are access is sufficient. Organizations are often giving people tools without giving often giving people tools without giving often giving people tools without giving them or their agents the organizational them or their agents the organizational them or their agents the organizational context and data that allow those tools context and data that allow those tools context and data that allow those tools to deliver real value. And that's where to deliver real value. And that's where to deliver real value. And that's where Clara's story intersects with co-pilot Clara's story intersects with co-pilot Clara's story intersects with co-pilot story. Tools deployed without story. Tools deployed without story. Tools deployed without organizational infrastructure become organizational infrastructure become organizational infrastructure become very expensive toys. The 74% of very expensive toys. The 74% of very expensive toys. The 74% of companies reporting no tangible value companies reporting no tangible value companies reporting no tangible value from AI are probably not failing because from AI are probably not failing because from AI are probably not failing because of models. They're failing because of models. They're failing because of models. They're failing because there's no shared understanding of how there's no shared understanding of how there's no shared understanding of how AI tools connect to organizational AI tools connect to organizational AI tools connect to organizational workflows, of where AI automation should workflows, of where AI automation should workflows, of where AI automation should replace human effort, of where it should replace human effort, of where it should replace human effort, of where it should augment it, of where human judgment augment it, of where human judgment augment it, of where human judgment should be non-negotiable, all the things should be non-negotiable, all the things should be non-negotiable, all the things that CLA should have done. All the that CLA should have done. All the that CLA should have done. All the things that the co-pilot salespeople things that the co-pilot salespeople things that the co-pilot salespeople didn't tell you about. This is the issue didn't tell you about. This is the issue didn't tell you about. This is the issue today with AI in the enterprise. We are today with AI in the enterprise. We are today with AI in the enterprise. We are not taking that data and context layer not taking that data and context layer not taking that data and context layer seriously. And that doesn't allow us to seriously. And that doesn't allow us to seriously. And that doesn't allow us to even approach layer three, which we're even approach layer three, which we're even approach layer three, which we're going to talk about next. Intent going to talk about next. Intent going to talk about next. Intent engineering proper. This is the layer engineering proper. This is the layer engineering proper. This is the layer that almost certainly doesn't exist in that almost certainly doesn't exist in that almost certainly doesn't exist in your business. It's the one I think your business. It's the one I think your business. It's the one I think matters the most, and it requires matters the most, and it requires matters the most, and it requires something genuinely new. OKRs were something genuinely new. OKRs were something genuinely new. OKRs were designed for people. They encode human designed for people. They encode human designed for people. They encode human readable goals. They assume human readable goals. They assume human readable goals. They assume human judgment about prioritization, judgment about prioritization, judgment about prioritization, trade-offs, values, and exceptions. They trade-offs, values, and exceptions. They trade-offs, values, and exceptions. They assume a manager can look a direct assume a manager can look a direct assume a manager can look a direct report in the eye and say, "Here's what report in the eye and say, "Here's what report in the eye and say, "Here's what matters this quarter." and trust that matters this quarter." and trust that matters this quarter." and trust that the report will interpret that guidance the report will interpret that guidance the report will interpret that guidance through a mesh of institutional context, through a mesh of institutional context, through a mesh of institutional context, professional norms, and personal professional norms, and personal professional norms, and personal judgment developed over months and judgment developed over months and judgment developed over months and years. Agents don't have any of that. An years. Agents don't have any of that. An years. Agents don't have any of that. An agent does not know your company's OKRs agent does not know your company's OKRs agent does not know your company's OKRs unless you put them in the context unless you put them in the context unless you put them in the context window. It doesn't know which trade-offs window. It doesn't know which trade-offs window. It doesn't know which trade-offs your leadership team would prefer unless your leadership team would prefer unless your leadership team would prefer unless you encode those preferences in a way it you encode those preferences in a way it you encode those preferences in a way it can act on. It doesn't know the can act on. It doesn't know the can act on. It doesn't know the difference between a decision that difference between a decision that difference between a decision that should be escalated and one it should should be escalated and one it should should be escalated and one it should make autonomously unless you define the make autonomously unless you define the make autonomously unless you define the boundary. And unlike a human employee, boundary. And unlike a human employee, boundary. And unlike a human employee, an agent will not absorb your company an agent will not absorb your company an agent will not absorb your company culture through osmosis for 6 months, culture through osmosis for 6 months, culture through osmosis for 6 months, through all hands meetings, through through all hands meetings, through through all hands meetings, through hallway conversations, and through hallway conversations, and through hallway conversations, and through watching senior people handle ambiguous watching senior people handle ambiguous watching senior people handle ambiguous situations. When a human employee joins situations. When a human employee joins situations. When a human employee joins a company, alignment happens through a a company, alignment happens through a a company, alignment happens through a 100 informal mechanisms. You read the 100 informal mechanisms. You read the 100 informal mechanisms. You read the wiki, you have a slack chat, you develop wiki, you have a slack chat, you develop wiki, you have a slack chat, you develop judgment, you have a happy hour with judgment, you have a happy hour with judgment, you have a happy hour with someone. None of that works for agents. someone. None of that works for agents. someone. None of that works for agents. Agents need explicit alignment, and they Agents need explicit alignment, and they Agents need explicit alignment, and they need it before they start working, not 6 need it before they start working, not 6 need it before they start working, not 6 months after. This means organizations months after. This means organizations months after. This means organizations need to develop something that mostly need to develop something that mostly need to develop something that mostly doesn't exist. Machine readable doesn't exist. Machine readable doesn't exist. Machine readable expressions of organizational intent. expressions of organizational intent. expressions of organizational intent. Think about what that requires. It is Think about what that requires. It is Think about what that requires. It is not just put the OKRs in the prompt. It not just put the OKRs in the prompt. It not just put the OKRs in the prompt. It is a cascade of specificity that most is a cascade of specificity that most is a cascade of specificity that most organizations have never had to produce organizations have never had to produce organizations have never had to produce because humans could fill in the gap. It because humans could fill in the gap. It because humans could fill in the gap. It is a cascade of specificity that is a cascade of specificity that is a cascade of specificity that organizations have never had to produce organizations have never had to produce organizations have never had to produce because humans could fill in the gaps. because humans could fill in the gaps. because humans could fill in the gaps. At the top, you need goal structures At the top, you need goal structures At the top, you need goal structures that agents can interpret and act on, that agents can interpret and act on, that agents can interpret and act on, not increase customer satisfaction. not increase customer satisfaction. not increase customer satisfaction. That's a human readable aspiration. You That's a human readable aspiration. You That's a human readable aspiration. You need an agent actionable objective. An need an agent actionable objective. An need an agent actionable objective. An agent needs to know what signals agent needs to know what signals agent needs to know what signals indicate customer satisfaction in our indicate customer satisfaction in our indicate customer satisfaction in our context. What data sources contain those context. What data sources contain those context. What data sources contain those signals? What actions am I authorized to signals? What actions am I authorized to signals? What actions am I authorized to take to improve them? What trade-offs am take to improve them? What trade-offs am take to improve them? What trade-offs am I empowered to make? speed versus I empowered to make? speed versus I empowered to make? speed versus thoroughess, cost versus quality, and thoroughess, cost versus quality, and thoroughess, cost versus quality, and where are the hard boundaries I may not where are the hard boundaries I may not where are the hard boundaries I may not cross. Below that, you need what I would cross. Below that, you need what I would cross. Below that, you need what I would call delegation frameworks, tenants call delegation frameworks, tenants call delegation frameworks, tenants translated into decision boundaries. translated into decision boundaries. translated into decision boundaries. Amazon's leadership principles work for Amazon's leadership principles work for Amazon's leadership principles work for humans because humans can interpret humans because humans can interpret humans because humans can interpret customer obsession through contextual customer obsession through contextual customer obsession through contextual judgment. An agent needs that principle judgment. An agent needs that principle judgment. An agent needs that principle to be decomposed. When customer request to be decomposed. When customer request to be decomposed. When customer request X conflicts with policy Y, here is the X conflicts with policy Y, here is the X conflicts with policy Y, here is the resolution hierarchy. When data suggests resolution hierarchy. When data suggests resolution hierarchy. When data suggests action A, but the customer expressed action A, but the customer expressed action A, but the customer expressed preference B, here's the decision logic. preference B, here's the decision logic. preference B, here's the decision logic. These are not rules in the traditional These are not rules in the traditional These are not rules in the traditional sense. They're encoded judgment. The sense. They're encoded judgment. The sense. They're encoded judgment. The kind of organizational knowledge that a kind of organizational knowledge that a kind of organizational knowledge that a senior employee carries in her head senior employee carries in her head senior employee carries in her head after 5 years and a new hire will absorb after 5 years and a new hire will absorb after 5 years and a new hire will absorb gradually. Agents need it now. And at gradually. Agents need it now. And at gradually. Agents need it now. And at the base, you need feedback mechanisms the base, you need feedback mechanisms the base, you need feedback mechanisms that actually close the loop. When an that actually close the loop. When an that actually close the loop. When an agent makes a decision, was it aligned agent makes a decision, was it aligned agent makes a decision, was it aligned with organizational intent? How do we with organizational intent? How do we with organizational intent? How do we know? This is exactly what happened at know? This is exactly what happened at know? This is exactly what happened at CLA. The agent optimized for resolution CLA. The agent optimized for resolution CLA. The agent optimized for resolution speed because that was the objective it speed because that was the objective it speed because that was the objective it could measure. Nobody had encoded the could measure. Nobody had encoded the could measure. Nobody had encoded the objectives that mattered most. objectives that mattered most. objectives that mattered most. Relationship quality, brand trust, Relationship quality, brand trust, Relationship quality, brand trust, customer lifetime value, the contextual customer lifetime value, the contextual customer lifetime value, the contextual judgment about when to be efficient and judgment about when to be efficient and judgment about when to be efficient and when to be generous. Those objectives when to be generous. Those objectives when to be generous. Those objectives lived in the heads of the human agents lived in the heads of the human agents lived in the heads of the human agents who had to walk out the door because who had to walk out the door because who had to walk out the door because they were fired. The age of humans just they were fired. The age of humans just they were fired. The age of humans just know is ending. Intent engineering is know is ending. Intent engineering is know is ending. Intent engineering is the discipline of making what humans the discipline of making what humans the discipline of making what humans know explicit, structured, and machine know explicit, structured, and machine know explicit, structured, and machine actionable. Not because the humans are actionable. Not because the humans are actionable. Not because the humans are leaving, although some of them will, but leaving, although some of them will, but leaving, although some of them will, but because the agents arriving to work because the agents arriving to work because the agents arriving to work alongside the people cannot function alongside the people cannot function alongside the people cannot function without it. If there is anything I want without it. If there is anything I want without it. If there is anything I want you to take away from this video, it is you to take away from this video, it is you to take away from this video, it is not if I can do this intent engineering, not if I can do this intent engineering, not if I can do this intent engineering, I can get rid of the people. You should I can get rid of the people. You should I can get rid of the people. You should be regarding agents as rather be regarding agents as rather be regarding agents as rather undependable actors and recognizing that undependable actors and recognizing that undependable actors and recognizing that you need humans to both encode intent you need humans to both encode intent you need humans to both encode intent engineering and maintain successful engineering and maintain successful engineering and maintain successful agentic systems that scale. That's how agentic systems that scale. That's how agentic systems that scale. That's how you actually start to drive agents in you actually start to drive agents in you actually start to drive agents in production. So why hasn't this been production. So why hasn't this been production. So why hasn't this been built yet? First, it's genuinely new. built yet? First, it's genuinely new. built yet? First, it's genuinely new. Before agents could run autonomously Before agents could run autonomously Before agents could run autonomously over long time horizons, we did not need over long time horizons, we did not need over long time horizons, we did not need this. The human was the intent layer. this. The human was the intent layer. this. The human was the intent layer. The agent never needed to understand The agent never needed to understand The agent never needed to understand organizational intent because you were organizational intent because you were organizational intent because you were standing right there. Longrunning agents standing right there. Longrunning agents standing right there. Longrunning agents break the model and demand a new way of break the model and demand a new way of break the model and demand a new way of thinking. And that's what this video is thinking. And that's what this video is thinking. And that's what this video is about. Second, the people who understand about. Second, the people who understand about. Second, the people who understand organizational strategy like executives organizational strategy like executives organizational strategy like executives are not the people who build agents. And are not the people who build agents. And are not the people who build agents. And the people building agents like the people building agents like the people building agents like engineers are not the people who engineers are not the people who engineers are not the people who understand organizational strategy very understand organizational strategy very understand organizational strategy very frequently. This is a classic two frequently. This is a classic two frequently. This is a classic two cultures problem. And it's acute in AI cultures problem. And it's acute in AI cultures problem. And it's acute in AI because the technology is moving so fast because the technology is moving so fast because the technology is moving so fast that the organizational thinkers cannot that the organizational thinkers cannot that the organizational thinkers cannot keep up and the technologists, they keep up and the technologists, they keep up and the technologists, they don't think it's their job. MIT found don't think it's their job. MIT found don't think it's their job. MIT found that AI investment is still viewed that AI investment is still viewed that AI investment is still viewed primarily as a tech challenge for the primarily as a tech challenge for the primarily as a tech challenge for the CIO rather than a business issue that CIO rather than a business issue that CIO rather than a business issue that requires leadership across the requires leadership across the requires leadership across the organization. That framing that's going organization. That framing that's going organization. That framing that's going to guarantee an intent gap that has real to guarantee an intent gap that has real to guarantee an intent gap that has real implications for your AI agents. CIOS implications for your AI agents. CIOS implications for your AI agents. CIOS can build infrastructure, but intent can build infrastructure, but intent can build infrastructure, but intent comes from the entire leadership team comes from the entire leadership team comes from the entire leadership team working together. The people who working together. The people who working together. The people who actually decide what the organization actually decide what the organization actually decide what the organization values and how it makes trade-offs need values and how it makes trade-offs need values and how it makes trade-offs need to be talking with engineering more. to be talking with engineering more. to be talking with engineering more. Third, just really hard. Making Third, just really hard. Making Third, just really hard. Making organizational intent explicit and organizational intent explicit and organizational intent explicit and structured is extremely difficult. Most structured is extremely difficult. Most structured is extremely difficult. Most organizations have never had to do this. organizations have never had to do this. organizations have never had to do this. Their goals live in slide decks, in OKR Their goals live in slide decks, in OKR Their goals live in slide decks, in OKR documents that get half read and documents that get half read and documents that get half read and referenced at personal reviews once a referenced at personal reviews once a referenced at personal reviews once a year, in leadership principles that get year, in leadership principles that get year, in leadership principles that get cited in performance reviews, but really cited in performance reviews, but really cited in performance reviews, but really they don't get operationalized in the they don't get operationalized in the they don't get operationalized in the tacet knowledge of experienced employees tacet knowledge of experienced employees tacet knowledge of experienced employees who know what to do in ambiguous who know what to do in ambiguous who know what to do in ambiguous situations even though they've never situations even though they've never situations even though they've never been told. Nobody has strong muscles been told. Nobody has strong muscles been told. Nobody has strong muscles here because most organizations have here because most organizations have here because most organizations have never exercised them. So what does a never exercised them. So what does a never exercised them. So what does a solution look like? I don't want to just solution look like? I don't want to just solution look like? I don't want to just leave you with a gap. First, at the leave you with a gap. First, at the leave you with a gap. First, at the infrastructure level, you need to infrastructure level, you need to infrastructure level, you need to develop a composable vendor agnostic develop a composable vendor agnostic develop a composable vendor agnostic architecture that enables agents to architecture that enables agents to architecture that enables agents to operate across systems, tools, and operate across systems, tools, and operate across systems, tools, and models securely and at scale. MCP is a models securely and at scale. MCP is a models securely and at scale. MCP is a sample protocol layer for this. But the sample protocol layer for this. But the sample protocol layer for this. But the organizational implementation requires organizational implementation requires organizational implementation requires decisions about data governance, access decisions about data governance, access decisions about data governance, access controls, freshness guarantees, and controls, freshness guarantees, and controls, freshness guarantees, and semantic consistency that no one semantic consistency that no one semantic consistency that no one protocol is going to make for you. The protocol is going to make for you. The protocol is going to make for you. The companies that build this well will companies that build this well will companies that build this well will treat it like they treated their data treat it like they treated their data treat it like they treated their data warehouse strategy in the as a core warehouse strategy in the as a core warehouse strategy in the as a core strategic investment not just an IT strategic investment not just an IT strategic investment not just an IT project. At the workflow level you need project. At the workflow level you need project. At the workflow level you need what I would call an organizational what I would call an organizational what I would call an organizational capability map for AI. A shared living capability map for AI. A shared living capability map for AI. A shared living understanding of which workflows are understanding of which workflows are understanding of which workflows are agent ready which are agent augmented agent ready which are agent augmented agent ready which are agent augmented with human in the loop and which remain with human in the loop and which remain with human in the loop and which remain human only. This is not a static human only. This is not a static human only. This is not a static document that gets filed in confluence document that gets filed in confluence document that gets filed in confluence and dumped. It's an operating system and dumped. It's an operating system and dumped. It's an operating system that evolves as agent capabilities keep that evolves as agent capabilities keep that evolves as agent capabilities keep improving and as organizational context improving and as organizational context improving and as organizational context infrastructure matures. The companies infrastructure matures. The companies infrastructure matures. The companies that do this well are likely going to be that do this well are likely going to be that do this well are likely going to be creating a new role. It will be called creating a new role. It will be called creating a new role. It will be called something like an AI workflow architect something like an AI workflow architect something like an AI workflow architect and it will sit between engineering and it will sit between engineering and it will sit between engineering operations and strategy and that person operations and strategy and that person operations and strategy and that person is going to be very busy at the is going to be very busy at the is going to be very busy at the alignment level. You need the genuinely alignment level. You need the genuinely alignment level. You need the genuinely new thing. Goal translation new thing. Goal translation new thing. Goal translation infrastructure that converts human infrastructure that converts human infrastructure that converts human readable organizational objectives into readable organizational objectives into readable organizational objectives into agent actionable parameters. This agent actionable parameters. This agent actionable parameters. This includes decision boundaries, includes decision boundaries, includes decision boundaries, escalation, value hierarchies like how escalation, value hierarchies like how escalation, value hierarchies like how the agent resolves trade-offs and the agent resolves trade-offs and the agent resolves trade-offs and feedback loops. How you measure and feedback loops. How you measure and feedback loops. How you measure and correct alignment drift over time. correct alignment drift over time. correct alignment drift over time. Google's agent development kit is one of Google's agent development kit is one of Google's agent development kit is one of the earliest attempts to formalize this the earliest attempts to formalize this the earliest attempts to formalize this at a technical level. It separates agent at a technical level. It separates agent at a technical level. It separates agent context into distinct layers. working context into distinct layers. working context into distinct layers. working context, session memory, long-term context, session memory, long-term context, session memory, long-term memory, and artifacts. Each of these has memory, and artifacts. Each of these has memory, and artifacts. Each of these has specific governance. There's also specific governance. There's also specific governance. There's also emerging academic work. A recent paper emerging academic work. A recent paper emerging academic work. A recent paper from researchers at Google DeepMind from researchers at Google DeepMind from researchers at Google DeepMind proposed five levels of AI agent proposed five levels of AI agent proposed five levels of AI agent autonomy. Operator, collaborator, autonomy. Operator, collaborator, autonomy. Operator, collaborator, consultant, approver, and observer, each consultant, approver, and observer, each consultant, approver, and observer, each with different intent alignment with different intent alignment with different intent alignment requirements and different human requirements and different human requirements and different human oversight models. These are just the oversight models. These are just the oversight models. These are just the early sketches. The integrated system is early sketches. The integrated system is early sketches. The integrated system is really whites space. Building context really whites space. Building context really whites space. Building context infrastructure plus workflow mapping infrastructure plus workflow mapping infrastructure plus workflow mapping plus intent alignment is new and it's an plus intent alignment is new and it's an plus intent alignment is new and it's an enormous challenge. If OKRs were the enormous challenge. If OKRs were the enormous challenge. If OKRs were the management innovation that let Intel management innovation that let Intel management innovation that let Intel align thousands of humans to shared align thousands of humans to shared align thousands of humans to shared objectives in the 70s, intent objectives in the 70s, intent objectives in the 70s, intent engineering is the management innovation engineering is the management innovation engineering is the management innovation that lets organizations align hundreds that lets organizations align hundreds that lets organizations align hundreds or thousands or tens of thousands of or thousands or tens of thousands of or thousands or tens of thousands of agents to those same objectives in 2026. agents to those same objectives in 2026. agents to those same objectives in 2026. While those agents operate at speeds and While those agents operate at speeds and While those agents operate at speeds and scales, no human manager can supervise. scales, no human manager can supervise. scales, no human manager can supervise. The parallel is direct. The urgency has The parallel is direct. The urgency has The parallel is direct. The urgency has never been greater. OKRs have taken never been greater. OKRs have taken never been greater. OKRs have taken decades to become standard management decades to become standard management decades to become standard management practice. We do not have 20 years to practice. We do not have 20 years to practice. We do not have 20 years to wait. For the past 3 years, the AI race wait. For the past 3 years, the AI race wait. For the past 3 years, the AI race has been framed as an intelligence race. has been framed as an intelligence race. has been framed as an intelligence race. Who has the best model? Who tops the Who has the best model? Who tops the Who has the best model? Who tops the best benchmarks? Who has the biggest best benchmarks? Who has the biggest best benchmarks? Who has the biggest context window? That framing made sense context window? That framing made sense context window? That framing made sense when models were a bottleneck. But when models were a bottleneck. But when models were a bottleneck. But models are not the bottleneck today. Not models are not the bottleneck today. Not models are not the bottleneck today. Not for most organizational use cases. The for most organizational use cases. The for most organizational use cases. The frontier models like Opus 4.6 or Gemini frontier models like Opus 4.6 or Gemini frontier models like Opus 4.6 or Gemini 3 or GPT 5.2. These are all 3 or GPT 5.2. These are all 3 or GPT 5.2. These are all extraordinarily capable models. The extraordinarily capable models. The extraordinarily capable models. The differences between them matter far less differences between them matter far less differences between them matter far less than the differences between than the differences between than the differences between organizations that give them clear, organizations that give them clear, organizations that give them clear, structured, goal-igned intent and structured, goal-igned intent and structured, goal-igned intent and organizations that don't. The race is an organizations that don't. The race is an organizations that don't. The race is an intent race. Not who has the smartest AI intent race. Not who has the smartest AI intent race. Not who has the smartest AI in their systems, but who has built the in their systems, but who has built the in their systems, but who has built the organizational infrastructure that lets organizational infrastructure that lets organizational infrastructure that lets AI operate with the fullest, most AI operate with the fullest, most AI operate with the fullest, most accurate, most strategically correct accurate, most strategically correct accurate, most strategically correct understanding of what the organization understanding of what the organization understanding of what the organization is trying to accomplish. The company is trying to accomplish. The company is trying to accomplish. The company with a mediocre model and extraordinary with a mediocre model and extraordinary with a mediocre model and extraordinary organizational intent infrastructure organizational intent infrastructure organizational intent infrastructure will outperform the company with a will outperform the company with a will outperform the company with a frontier model and fragmented, frontier model and fragmented, frontier model and fragmented, inaccessible, unaligned organizational inaccessible, unaligned organizational inaccessible, unaligned organizational knowledge every single time. This means knowledge every single time. This means knowledge every single time. This means that the most important AI investment in that the most important AI investment in that the most important AI investment in 2026 isn't really a model subscription. 2026 isn't really a model subscription. 2026 isn't really a model subscription. It's not another co-pilot license. It's It's not another co-pilot license. It's It's not another co-pilot license. It's organizational intent architecture. organizational intent architecture. organizational intent architecture. Making your company's goals, values, Making your company's goals, values, Making your company's goals, values, decision frameworks, and trade-off decision frameworks, and trade-off decision frameworks, and trade-off hierarchies discoverable, structured, hierarchies discoverable, structured, hierarchies discoverable, structured, and agent actionable. It's building the and agent actionable. It's building the and agent actionable. It's building the alignment infrastructure that lets alignment infrastructure that lets alignment infrastructure that lets agents make decisions that aren't just agents make decisions that aren't just agents make decisions that aren't just technically correct, but that are technically correct, but that are technically correct, but that are strategically coherent. It's developing strategically coherent. It's developing strategically coherent. It's developing the shared language and shared systems the shared language and shared systems the shared language and shared systems that let AI capabilities scale from one that let AI capabilities scale from one that let AI capabilities scale from one heroic engineer to 40,000 knowledge heroic engineer to 40,000 knowledge heroic engineer to 40,000 knowledge workers operating in concert. Clara's workers operating in concert. Clara's workers operating in concert. Clara's story was not AI doesn't work. The AI story was not AI doesn't work. The AI story was not AI doesn't work. The AI worked brilliantly. That was the worked brilliantly. That was the worked brilliantly. That was the problem. It was so good at optimizing problem. It was so good at optimizing problem. It was so good at optimizing for the measurable objective that nobody for the measurable objective that nobody for the measurable objective that nobody noticed it was destroying the ones that noticed it was destroying the ones that noticed it was destroying the ones that really mattered. Trust. The 700 human really mattered. Trust. The 700 human really mattered. Trust. The 700 human agents that got laid off took with them agents that got laid off took with them agents that got laid off took with them the institutional knowledge that really the institutional knowledge that really the institutional knowledge that really mattered. The knowledge that had never mattered. The knowledge that had never mattered. The knowledge that had never been documented. Humans just knew. The been documented. Humans just knew. The been documented. Humans just knew. The lesson is to build the intent layer so lesson is to build the intent layer so lesson is to build the intent layer so that agents don't need to absorb that agents don't need to absorb that agents don't need to absorb organizational values through osmosis organizational values through osmosis organizational values through osmosis because they can't. The lesson is to because they can't. The lesson is to because they can't. The lesson is to recognize that agents need humans recognize that agents need humans recognize that agents need humans working alongside them. Maybe Clara has working alongside them. Maybe Clara has working alongside them. Maybe Clara has finally learned that the prompt finally learned that the prompt finally learned that the prompt engineering era asked, "How do I talk to engineering era asked, "How do I talk to engineering era asked, "How do I talk to AI?" The context engineering era is AI?" The context engineering era is AI?" The context engineering era is asking us now, "What does AI need to asking us now, "What does AI need to asking us now, "What does AI need to know?" And the intent engineering era is know?" And the intent engineering era is know?" And the intent engineering era is beginning to ask us the question that beginning to ask us the question that beginning to ask us the question that really matters. What does the really matters. What does the really matters. What does the organization need AI to want to be organization need AI to want to be organization need AI to want to be productive? Context without intent is productive? Context without intent is productive? Context without intent is like a loaded weapon with no target. like a loaded weapon with no target. like a loaded weapon with no target. We've spent years building AI systems. We've spent years building AI systems. We've spent years building AI systems. 2026 is a year when we learn to aim them 2026 is a year when we learn to aim them 2026 is a year when we learn to aim them toward an organizational intent that toward an organizational intent that toward an organizational intent that really matters. If you're listening to really matters. If you're listening to really matters. If you're listening to this, you are involved at one or more of this, you are involved at one or more of this, you are involved at one or more of these layers. Everybody is from the these layers. Everybody is from the these layers. Everybody is from the individual contributors who are working individual contributors who are working individual contributors who are working against these systems and practicing against these systems and practicing against these systems and practicing prompting and trying to use them to prompting and trying to use them to prompting and trying to use them to gather context all the way up to the gather context all the way up to the gather context all the way up to the systems designers and all the way up to systems designers and all the way up to systems designers and all the way up to the seauite. It is up to all of us to the seauite. It is up to all of us to the seauite. It is up to all of us to build layers that enable agents to act build layers that enable agents to act build layers that enable agents to act productively in line with organizational productively in line with organizational productively in line with organizational values. If we are not careful, failure values. If we are not careful, failure values. If we are not careful, failure to do so is going to lead to AI agents to do so is going to lead to AI agents to do so is going to lead to AI agents that cause active harm to the business. that cause active harm to the business. that cause active harm to the business. That's what Clarno learned. Don't do That's what Clarno learned. Don't do That's what Clarno learned. Don't do that. Build systems that encode both that. Build systems that encode both that. Build systems that encode both context and intent at the organizational context and intent at the organizational context and intent at the organizational scale. The clock is running. And the scale. The clock is running. And the scale. The clock is running. And the teams that do this are going to be able teams that do this are going to be able teams that do this are going to be able to unleash the power of the agents that to unleash the power of the agents that to unleash the power of the agents that are running for weeks and soon for are running for weeks and soon for are running for weeks and soon for months with a whole lot more confidence months with a whole lot more confidence months with a whole lot more confidence than the people who are building systems than the people who are building systems than the people who are building systems where they don't encode intent, where where they don't encode intent, where where they don't encode intent, where they don't encode values, where they they don't encode values, where they they don't encode values, where they don't encode tradeoffs, where you cannot don't encode tradeoffs, where you cannot don't encode tradeoffs, where you cannot trust an agent not to hang up on a trust an agent not to hang up on a trust an agent not to hang up on a customer just because you told them to customer just because you told them to customer just because you told them to make the call shorter. Don't do that. make the call shorter. Don't do that. make the call shorter. Don't do that. Build for long-term intent because Build for long-term intent because Build for long-term intent because agents with long-term intent are
