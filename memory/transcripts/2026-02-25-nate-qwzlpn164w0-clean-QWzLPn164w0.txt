In January, CLA reported its AI agent now does the work of 853 full-time employees and has saved the company $60 million. In the same earnings cycle, its CEO admitted publicly that the AI strategy had cost something far more valuable than $60 million, and he's still trying to buy it back. This is not another AI is overhyped story. It is actually the opposite. The AI work too well. And the distinction between AI that fails and AI that succeeds at the wrong thing is the most important unsolved problem in enterprise AI right now. This is a bigger problem than context engineering per se, although that's a piece of it. It's bigger than prompt engineering, which now frankly looks like a warm-up act. I'm going to call what we're talking about here intent engineering. It is the discipline of making organizational purpose like goals, values, tradeoffs, decision boundaries. These need to become machine readable and machine actionable so that when you deploy an autonomous system, it optimizes for what your company actually needs, not just what it can measure. Here's the CLA backstory. In early 2024, Ara rolled out an AI powered customer service agent. It handled 2.3 million conversations in the first month across 23 markets in 35 languages. Resolution times dropped from 11 minutes to two. The CEO projected $40 million in savings. And then customers started complaining. Generic answers, robotic tone, no ability to handle anything requiring judgment. By mid 2025, CEO Sebastian Seycowski told Bloomberg that while cost was a predominant evaluation factor, the result was lower quality. End quote. Clarin began frantically rehiring the human agents it had gutted. Most people tell this story as proof that AI can't handle nuance, and that was a comforting reading in early 2025. A more interesting reading in 2026 is that the AI agent was extraordinarily good at resolving tickets fast and that that was the wrong goal to give the agent. Clara's organizational intent wasn't resolve tickets fast. It was actually build lasting customer relationships that drive lifetime value in a very competitive fintech market. Those are profoundly different goals and they require profoundly different decision-making at the point of interaction. A human agent with five years at the company knows this difference intuitively. She knows when to bend a policy, when to spend three extra minutes because the customer's tone says they're about to churn, when efficiency is the right move versus when generosity is the right move. She knows this because she absorbed Clara's real values. Not the ones on the website, but the ones encoded in the decisions managers make every day in the stories veterans tell new hires. In the unwritten rules about which metrics leadership actually cares about when Bush comes to shove, the AI agent knew none of it. It had a prompt. It had context. It did not have intent. I am concerned that the AI agent inadvertently reflected the real values of CLA when it behaved the way it did because the real values of CLA may have been to save the money first. Nevertheless, customers pushed back and called CLA back toward its own stated values. And that's been a good thing because the $60 million in savings since the program rolled out have been not nearly enough to cover the reputational damage to CLA from becoming a public laughingtock over AI and from aggressively pushing customer service resolutions that did not meet customer needs. Ironically, rolling out an AI without much thought, but perhaps in line with organizational directives, did more to push CLA toward its own stated values than perhaps anything else would have done. I want to be precise about this story. The point here is not to talk about CLA per se. The point is to talk about what our organizational intent is with AI agents, what our goals are, and how agents need to reflect the larger perspective and longer term challenge of our organizations as they become more sophisticated and operate for longer time scales themselves. We have agents now that run for multiple weeks. We have agents soon that will run for multiple months. We are at a level where it is time to think about how agents interact with organizational goals very seriously and CLA is just kind of an example of why that's important. I want to be precise about how we got here because naming things matters. Naming is how we create a shared understanding and I think we are short on naming things correctly when it comes to intent and context. Prompt engineering was the first discipline in the age of AI. It was individual, synchronous, and sessionbased. You sit in front of the chat window, you craft an instruction, you iterate the output. It's a personal skill, and the value is personal. This is the era that produced a thousand how to write the perfect prompt blog post. Most of them are terrible. Context engineering followed prompt engineering. It's the one the industry is currently grappling with. Anthropic published a foundational piece in September of 2025 that defined context engineering is the shift from crafting isolated instructions to crafting the entire information state that an AI system operates within. Chains Harrison Chase put it more bluntly in a Sequoia Capital interview when he described it as everything's context engineering. Context engineering is such a good term. I wish I came up with that term because it describes everything we've done at Langchain without knowing the term existed. End quote. That's pretty good. Context engineering is where the action is right now. Building rag pipelines, wiring up MCP servers, structuring organizational knowledge so agents can access it. It's necessary, but it's not sufficient. And I think the industry is about to discover that in a very expensive way. Intent engineering. Intent engineering is the third discipline and it's the one that almost nobody's building for yet. Context engineering tells the agents what to know. Intent engineering tells agents what to want. It's the practice of encoding organizational purpose into infrastructure, not as pros in a system prompt, but as structured, actionable parameters that shape how agents make decisions autonomously. It's the layer that would have told Clara's AI agent, "Yes, you can resolve this ticket in 90 seconds, but the customer has been with us for years, and their tone indicates frustration. Spend the extra time. Offer them a specialist." The goal is retention. Without intent engineering, you get what Claragot, a technically brilliant agent optimizing for exactly the wrong objective. You get what Deoid's 2026 state of AI in the enterprise report found across 3,000 some leaders in 24 countries. 84% of companies have not redesigned jobs around AI capabilities and only 21% have a mature model for agent governance. These numbers aren't a technology story. They're an intent failure. The models work. The context pipelines are getting better. What's missing is the organizational infrastructure that connects AI capability to organizational purpose. I cited the failure stats from deote above. I want to show you the other side of the ledger because the juxtaposition makes this all very disorienting. Investment in AI continues to be massive and accelerating. Deoid's tech value survey found that 57% of respondents were putting between 21 and 50% of their digital transformation budgets into AI automation. and 20% of companies were investing over half on average $700 million for a company with 13 billion in revenue. KPMG's Q4 AI pulse survey showed capital flowing ROI confidence rising in agents moving from pilots to professionalized platforms. Gartner is predicting that by 2028 15% of dayto-day work decisions will be made autonomously by agents. I think that might be low. So the money is real, the deployments are real, and the results are very much in between. 74% of companies globally report they have yet to see tangible value from AI. McKenzie found 30% of AI pilots failed to achieve scaled impact. These numbers all coexist together with the investment numbers. There's not really a contradiction here if you start to peel the onion and understand things more carefully. What we're describing when we talk about a pattern of scaled investment and somewhat mixed results on deployment is that organizations have solved can AI do this task at an individual task level and they have completely failed to solve can AI do this task in a way that serves our organizational goals at scale with appropriate judgment. That second question that's an intent engineering question. Look at what happened with Microsoft Copilot. One of the most heavily invested enterprise AI products in history. Microsoft poured billions into infrastructure, embedded AI into every office application, and launched an aggressive enterprise sales campaign. 85% of Fortune 500 companies adopted it, and the adoption stalled hard. Gartner found that only 5% of organizations moved from a co-pilot pilot to a larger scale deployment. Only about 3% of the total Microsoft 365 user base actually adopted Copilot as paid users. Bloomberg reported Microsoft slashing internal sales targets after the majority of salespeople missed their goals. Even inside companies that signed six figure co-pilot deals, employees resisted. Reddit threads are full of engineers at multi-billion dollar companies describing their organizations downgrading licenses because employees preferred another AI. maybe chat GPT, maybe Claude. The standard explanation for co-pilot struggles centers on UX problems and model quality. And those are definitely real issues, but they're not the fundamental issue. The fundamental issue is that deploying an AI tool across an organization without organizational intent alignment is like hiring 40,000 new employees and never telling them what the company does, what it values, or how to make decisions. You get lots of activity and not much productivity. You get AI usage metrics in a dashboard and almost no measurable impact on what the organization is trying to accomplish. That's not a tools problem. That's an intent gap. I want to get structural because vague handwaving about AI transformation is exactly what we're trying to avoid here and what so many organizations get into trouble doing. There is an intent gap today and it operates across three distinct layers, each one at a different altitude. Getting any one of them right is helpful. Getting all three right is the difference between having AI tools and having an AI native organization. Layer one is what I'm going to call a unified context infrastructure. This is the layer the industry is most aware of and it's still not really built yet. Right now, every team building agents rolls their own context stack. One team pipes Slack data through a custom rag pipeline. Another manually exports Google Docs into a vector store. A third built an MCP server that connects to Salesforce but not to Jira. A fourth team doesn't know the other three exist yet. This is what one analyst called the shadow agents problem and it mirrors the shadow IT crisis of the early cloud era except the stakes are much higher because agents don't just access data, they act upon it. Security and compliance teams can't allow arbitrary unvetted agents running on developer laptops to access critical systems like customer PII, financial data or healthcare records. But without sanctioned infrastructure, that is exactly what is happening. The model context protocol which Anthropic introduced late in 2024 and donated to the Linux Foundation in December of 2025 is the most promising attempt at standardization at this point. UCP has seen a ton of adoption. OpenAI, Google, Microsoft, and more than 50 enterprise partners have committed to it. It's become the de facto standard. Monthly SDK downloads are close to 100 million now, I think. But protocol adoption and organizational implementation are very different things. Having a USBC standard does not help if your company hasn't decided which ports to install, who maintains them, or what gets plugged in. The context infrastructure question is not really a technical question. You can configure MCP servers. It is architectural and political. Which systems become agent accessible? Who decides what context an agent can see across departments? How do you version organizational knowledge so agents aren't operating on stale information? How do you handle the fact that the sales team Slack context and the engineering team Slack context encode completely different institutional assumptions? Deoid's 2025 survey found that nearly half of organizations cited data searchability and data reusability as top challenges blocking AI automation. I'm surprised that number isn't higher. As their analyst put it, the shift required is from traditional ETL data pipelines to enterprise search and indexing, similar to how Google made the worldwide web discover. The data does exist inside corporations. The agents also exist increasingly, but the connective tissue between them, the organizational context layer and the structures and safeguards to ensure that's accessed correctly, that mostly doesn't exist. Now, we're going to move on to layer two, the coherent AI worker toolkit. So, everyone's rolling out their own AI workflow. One person uses Claude for research and chat GPT for drafting. Another uses cursor for code and perplexity for factchecking. A third has built a custom agent chain using langraph. A fourth is copy pasting into a chat window. None of these employees can articulate their workflow in a way that's transferable, measurable, or improvable by anybody else. And this matters because the difference between individual AI use and organizational AI leverage is enormous. It's the difference between having one good hire and having a system that makes everybody better. It's the difference I've been writing about for a year between AI activity and AI fluency. The former has 30% gains that you get from bolting AI onto existing workflows. And the latter has the 300% gains you get from rethinking the workflow itself around AI capabilities. But here's what we need to realize. Fluency doesn't scale through training alone. It scales through shared infrastructure. Whether any individual person has Slack doesn't matter. Whether an agent can search 50 people's Slack context plus their docs plus their project plan plus the customer data, that's what determines whether the agent can do organizational scale work rather than individual scale tasks. Lloyd's 2026 report found that workforce access to sanctioned AI tools expanded by 50% in a year. But that doesn't mean that access is sufficient. Organizations are often giving people tools without giving them or their agents the organizational context and data that allow those tools to deliver real value. And that's where Clara's story intersects with co-pilot story. Tools deployed without organizational infrastructure become very expensive toys. The 74% of companies reporting no tangible value from AI are probably not failing because of models. They're failing because there's no shared understanding of how AI tools connect to organizational workflows, of where AI automation should replace human effort, of where it should augment it, of where human judgment should be non-negotiable, all the things that CLA should have done. All the things that the co-pilot salespeople didn't tell you about. This is the issue today with AI in the enterprise. We are not taking that data and context layer seriously. And that doesn't allow us to even approach layer three, which we're going to talk about next. Intent engineering proper. This is the layer that almost certainly doesn't exist in your business. It's the one I think matters the most, and it requires something genuinely new. OKRs were designed for people. They encode human readable goals. They assume human judgment about prioritization, trade-offs, values, and exceptions. They assume a manager can look a direct report in the eye and say, "Here's what matters this quarter." and trust that the report will interpret that guidance through a mesh of institutional context, professional norms, and personal judgment developed over months and years. Agents don't have any of that. An agent does not know your company's OKRs unless you put them in the context window. It doesn't know which trade-offs your leadership team would prefer unless you encode those preferences in a way it can act on. It doesn't know the difference between a decision that should be escalated and one it should make autonomously unless you define the boundary. And unlike a human employee, an agent will not absorb your company culture through osmosis for 6 months, through all hands meetings, through hallway conversations, and through watching senior people handle ambiguous situations. When a human employee joins a company, alignment happens through a 100 informal mechanisms. You read the wiki, you have a slack chat, you develop judgment, you have a happy hour with someone. None of that works for agents. Agents need explicit alignment, and they need it before they start working, not 6 months after. This means organizations need to develop something that mostly doesn't exist. Machine readable expressions of organizational intent. Think about what that requires. It is not just put the OKRs in the prompt. It is a cascade of specificity that most organizations have never had to produce because humans could fill in the gap. It is a cascade of specificity that organizations have never had to produce because humans could fill in the gaps. At the top, you need goal structures that agents can interpret and act on, not increase customer satisfaction. That's a human readable aspiration. You need an agent actionable objective. An agent needs to know what signals indicate customer satisfaction in our context. What data sources contain those signals? What actions am I authorized to take to improve them? What trade-offs am I empowered to make? speed versus thoroughess, cost versus quality, and where are the hard boundaries I may not cross. Below that, you need what I would call delegation frameworks, tenants translated into decision boundaries. Amazon's leadership principles work for humans because humans can interpret customer obsession through contextual judgment. An agent needs that principle to be decomposed. When customer request X conflicts with policy Y, here is the resolution hierarchy. When data suggests action A, but the customer expressed preference B, here's the decision logic. These are not rules in the traditional sense. They're encoded judgment. The kind of organizational knowledge that a senior employee carries in her head after 5 years and a new hire will absorb gradually. Agents need it now. And at the base, you need feedback mechanisms that actually close the loop. When an agent makes a decision, was it aligned with organizational intent? How do we know? This is exactly what happened at CLA. The agent optimized for resolution speed because that was the objective it could measure. Nobody had encoded the objectives that mattered most. Relationship quality, brand trust, customer lifetime value, the contextual judgment about when to be efficient and when to be generous. Those objectives lived in the heads of the human agents who had to walk out the door because they were fired. The age of humans just know is ending. Intent engineering is the discipline of making what humans know explicit, structured, and machine actionable. Not because the humans are leaving, although some of them will, but because the agents arriving to work alongside the people cannot function without it. If there is anything I want you to take away from this video, it is not if I can do this intent engineering, I can get rid of the people. You should be regarding agents as rather undependable actors and recognizing that you need humans to both encode intent engineering and maintain successful agentic systems that scale. That's how you actually start to drive agents in production. So why hasn't this been built yet? First, it's genuinely new. Before agents could run autonomously over long time horizons, we did not need this. The human was the intent layer. The agent never needed to understand organizational intent because you were standing right there. Longrunning agents break the model and demand a new way of thinking. And that's what this video is about. Second, the people who understand organizational strategy like executives are not the people who build agents. And the people building agents like engineers are not the people who understand organizational strategy very frequently. This is a classic two cultures problem. And it's acute in AI because the technology is moving so fast that the organizational thinkers cannot keep up and the technologists, they don't think it's their job. MIT found that AI investment is still viewed primarily as a tech challenge for the CIO rather than a business issue that requires leadership across the organization. That framing that's going to guarantee an intent gap that has real implications for your AI agents. CIOS can build infrastructure, but intent comes from the entire leadership team working together. The people who actually decide what the organization values and how it makes trade-offs need to be talking with engineering more. Third, just really hard. Making organizational intent explicit and structured is extremely difficult. Most organizations have never had to do this. Their goals live in slide decks, in OKR documents that get half read and referenced at personal reviews once a year, in leadership principles that get cited in performance reviews, but really they don't get operationalized in the tacet knowledge of experienced employees who know what to do in ambiguous situations even though they've never been told. Nobody has strong muscles here because most organizations have never exercised them. So what does a solution look like? I don't want to just leave you with a gap. First, at the infrastructure level, you need to develop a composable vendor agnostic architecture that enables agents to operate across systems, tools, and models securely and at scale. MCP is a sample protocol layer for this. But the organizational implementation requires decisions about data governance, access controls, freshness guarantees, and semantic consistency that no one protocol is going to make for you. The companies that build this well will treat it like they treated their data warehouse strategy in the as a core strategic investment not just an IT project. At the workflow level you need what I would call an organizational capability map for AI. A shared living understanding of which workflows are agent ready which are agent augmented with human in the loop and which remain human only. This is not a static document that gets filed in confluence and dumped. It's an operating system that evolves as agent capabilities keep improving and as organizational context infrastructure matures. The companies that do this well are likely going to be creating a new role. It will be called something like an AI workflow architect and it will sit between engineering operations and strategy and that person is going to be very busy at the alignment level. You need the genuinely new thing. Goal translation infrastructure that converts human readable organizational objectives into agent actionable parameters. This includes decision boundaries, escalation, value hierarchies like how the agent resolves trade-offs and feedback loops. How you measure and correct alignment drift over time. Google's agent development kit is one of the earliest attempts to formalize this at a technical level. It separates agent context into distinct layers. working context, session memory, long-term memory, and artifacts. Each of these has specific governance. There's also emerging academic work. A recent paper from researchers at Google DeepMind proposed five levels of AI agent autonomy. Operator, collaborator, consultant, approver, and observer, each with different intent alignment requirements and different human oversight models. These are just the early sketches. The integrated system is really whites space. Building context infrastructure plus workflow mapping plus intent alignment is new and it's an enormous challenge. If OKRs were the management innovation that let Intel align thousands of humans to shared objectives in the 70s, intent engineering is the management innovation that lets organizations align hundreds or thousands or tens of thousands of agents to those same objectives in 2026. While those agents operate at speeds and scales, no human manager can supervise. The parallel is direct. The urgency has never been greater. OKRs have taken decades to become standard management practice. We do not have 20 years to wait. For the past 3 years, the AI race has been framed as an intelligence race. Who has the best model? Who tops the best benchmarks? Who has the biggest context window? That framing made sense when models were a bottleneck. But models are not the bottleneck today. Not for most organizational use cases. The frontier models like Opus 4.6 or Gemini 3 or GPT 5.2. These are all extraordinarily capable models. The differences between them matter far less than the differences between organizations that give them clear, structured, goal-igned intent and organizations that don't. The race is an intent race. Not who has the smartest AI in their systems, but who has built the organizational infrastructure that lets AI operate with the fullest, most accurate, most strategically correct understanding of what the organization is trying to accomplish. The company with a mediocre model and extraordinary organizational intent infrastructure will outperform the company with a frontier model and fragmented, inaccessible, unaligned organizational knowledge every single time. This means that the most important AI investment in 2026 isn't really a model subscription. It's not another co-pilot license. It's organizational intent architecture. Making your company's goals, values, decision frameworks, and trade-off hierarchies discoverable, structured, and agent actionable. It's building the alignment infrastructure that lets agents make decisions that aren't just technically correct, but that are strategically coherent. It's developing the shared language and shared systems that let AI capabilities scale from one heroic engineer to 40,000 knowledge workers operating in concert. Clara's story was not AI doesn't work. The AI worked brilliantly. That was the problem. It was so good at optimizing for the measurable objective that nobody noticed it was destroying the ones that really mattered. Trust. The 700 human agents that got laid off took with them the institutional knowledge that really mattered. The knowledge that had never been documented. Humans just knew. The lesson is to build the intent layer so that agents don't need to absorb organizational values through osmosis because they can't. The lesson is to recognize that agents need humans working alongside them. Maybe Clara has finally learned that the prompt engineering era asked, "How do I talk to AI?" The context engineering era is asking us now, "What does AI need to know?" And the intent engineering era is beginning to ask us the question that really matters. What does the organization need AI to want to be productive? Context without intent is like a loaded weapon with no target. We've spent years building AI systems. 2026 is a year when we learn to aim them toward an organizational intent that really matters. If you're listening to this, you are involved at one or more of these layers. Everybody is from the individual contributors who are working against these systems and practicing prompting and trying to use them to gather context all the way up to the systems designers and all the way up to the seauite. It is up to all of us to build layers that enable agents to act productively in line with organizational values. If we are not careful, failure to do so is going to lead to AI agents that cause active harm to the business. That's what Clarno learned. Don't do that. Build systems that encode both context and intent at the organizational scale. The clock is running. And the teams that do this are going to be able to unleash the power of the agents that are running for weeks and soon for months with a whole lot more confidence than the people who are building systems where they don't encode intent, where they don't encode values, where they don't encode tradeoffs, where you cannot trust an agent not to hang up on a customer just because you told them to make the call shorter. Don't do that. Build for long-term intent because agents with long-term intent are
